%---------- Inleiding ---------------------------------------------------------

% TODO: Is dit voorstel gebaseerd op een paper van Research Methods die je
% vorig jaar hebt ingediend? Heb je daarbij eventueel samengewerkt met een
% andere student?
% Zo ja, haal dan de tekst hieronder uit commentaar en pas aan.

%\paragraph{Opmerking}

% Dit voorstel is gebaseerd op het onderzoeksvoorstel dat werd geschreven in het
% kader van het vak Research Methods dat ik (vorig/dit) academiejaar heb
% uitgewerkt (met medesturent VOORNAAM NAAM als mede-auteur).
% 

\section{Inleiding}%
\label{sec:inleiding}

De videogame-industrie is uitgegroeid tot een van de meest winstgevende sectoren binnen de entertainmentindustrie de afgelopen decennia. Het is zelfs zo gegroeid dat het de inkomsten overtreft van de film- en muziekindustrie. Deze groei gaat hand in hand met een toenemende complexiteit van games: niet-lineaire verhalen, multiplayerfunctionaliteiten en uitgebreide open werelden. Door deze complexiteit worden game-ontwikkelaars voortdurend op de proef gesteld op het gebied van kwaliteitsborging, waarbij de traditionele testmethoden meer onder druk komen te staan.



De doelgroep van dit onderzoek bestaat uit game-ontwikkelaars en quality assurance-teams, met focus op studios die met een beperkt budget en met veel tijdsdruk geconfronteerd worden. Zoals eerder vermeld is handmatig testen arbeidsintensief, vatbaar voor menselijke fouten en kostbaar. Het gevolg hiervan is dat het testproces een bottleneck vormt voor deze soort bedrijven. Daarentegen is automatisering moeilijk realiseerbaar, omdat games nu minder lineair en meer interactief zijn.



De probleemstelling die centraal staat in dit onderzoek is dat gametesten duur, tijdrovend en moeilijk volledig te automatiseren is door de complexiteit en niet-lineariteit van moderne games. Het is moeilijk voor een handmatige tester om alle verschillende scenario's en interacties te dekken. Dit zorgt ervoor dat testdekking inconsistent is en dat er een verhoogd risico is op bugs die pas na release ontdekt worden. Dit leidt toch tot mogelijke reputatieschade en ontevreden spelers. De centrale onderzoeksvraag luidt daarom: Hoe kan AI worden ingezet om speltesting efficiënter te maken zonder verlies aan testkwaliteit of bugdetectie?

%---------- Stand van zaken ---------------------------------------------------

\section{Literatuurstudie}%
\label{sec:literatuurstudie}

\subsection{Traditionele game testing}
Game testing verschilt fundamenteel van traditionele software testing door de niet-deterministische aard van gameplay en complexe interacties tussen systemen~\autocite{Politowski2021}. Handmatige testing blijft de dominante aanpak, hoewel dit gepaard gaat met hoge kosten en beperkte schaalbaarheid. Bestaande testing tools zijn vaak onvoldoende afgestemd op de specifieke behoeften van game development~\autocite{Politowski2022}. \textcite{Butt2023} tonen aan dat bepaalde categorieën bugs zoals collision detection en physics-gerelateerde problemen regelmatig gemist worden bij traditionele testmethoden. Geen universele oplossing bestaat; verschillende gamegenres vereisen specifieke testaanpakken~\autocite{Albaghajati2020}.

\subsection{Agent-based testing}
\textcite{Shirzadehhajimahmood2021} demonstreren dat agent-based testing veelbelovend is voor het systematisch verkennen van game states en het detecteren van edge cases. \textcite{Ariyurek2019} vergelijken synthetic agents (coverage-georiënteerd) met human-like agents (usability-gericht), wat wijst op het potentieel van een hybride aanpak. De IV4XR framework van \textcite{Prasetya2022} toonde aan dat AI-agents significant sneller kunnen testen dan menselijke testers, maar moeite hebben met het detecteren van subtiele gameplay mechanics en usability issues.

\subsection{Deep reinforcement learning}
Met deep reinforcement learning zijn agents in staat game environments zelfstandig te doorlopen zonder enige vorm van voorkennis. Vergelijkingen laten zien dat het Wuji-framework effectiever is in het opsporen van bugs dan traditionele scripted bots in online multiplayer games. In AAA-games stuiten implementaties echter op obstakels zoals uitgebreide trainingstijden en hoge eisen aan compute-resources. Reinforcement learning-agents hebben hun waarde bewezen voor zowel load testing als stress testing.

\subsection{Computer vision}
Computer vision methods bieden een alternatieve benadering door visuele game states te analyseren voor black-box testing~\autocite{Paduraru2021}. De keuze van AI-techniek hangt sterk af van gametype, ontwikkelingsstadium en beschikbare resources~\autocite{Zarembo2019}. Semi-automatische frameworks die menselijke expertise combineren met AI-capabilities worden beschouwd als essentieel~\autocite{Nantes2008}.

\subsection{Kennislacunes}
Bestaande literatuur biedt echter beperkte kwantitatieve vergelijkingen tussen AI-agents en menselijke testers in realistische industriële contexten. Dit onderzoek adresseert deze kennislacune door beide benaderingen systematisch te evalueren op testsnelheid, bugdetectie-effectiviteit en testdekking, met specifieke focus op praktische toepasbaarheid voor mid-size studios en indie-ontwikkelaars.

%---------- Methodologie ------------------------------------------------------
\section{Methodologie}%
\label{sec:methodologie}

In dit onderzoek proberen we een antwoord te vinden op de centrale onderzoeksvraag en de deelvragen die erbij horen. Het hele traject duurt twaalf weken, van 2 maart tot 22 mei 2026. Doorheen het proces is het vooral belangrijk dat we goed bijhouden wat we allemaal doen.

\subsection{Fase 1: Afbakening (week 1--2)}
\begin{description}
  \item[Onderzoekstechnieken:] We zoeken info op in de literatuur en praten kort met QA-medewerkers om helder te krijgen wat we precies moeten meten.
  \item[Doel:] Duidelijk krijgen wat we precies willen meten en onder welke omstandigheden, zodat we later alles makkelijk opnieuw kunnen testen.
  \item[Deliverables:] 
\end{description}
\begin{itemize}
  \item Een duidelijke bugdefinitie en indeling in types (crashes, softlocks, physics-problemen, performance-issues, \ldots).
  \item Hoe we beslissen dat iets een aparte bug is en hoe we die bijhouden.
  \item Randvoorwaarden zoals tijdslimieten, hardware, buildtype en welke scene/level we gebruiken.
\end{itemize}

\subsection{Fase 2: Testopstelling (week 3--4)}
\begin{description}
  \item[Onderzoekstechnieken:] Proof-of-concept opstelling + meetplan.
  \item[Doel:] Alles automatisch laten registreren in een testomgeving, zodat vergelijken later gemakkelijk is.
  \item[Deliverables:]
\end{description}
\begin{itemize}
  \item Logging (CSV/JSON), crashdetectie en automatische timeouts.
  \item Simpele test-orakels zoals assertions, loganalyse of performancegrenzen.
  \item Baselines om mee te vergelijken:
\begin{itemize}
  \item een menselijk testprotocol met beperkt tijdsbudget;
  \item een basic scripted bot voor regressietests.
\end{itemize}
\end{itemize}

\subsection{Fase 3: AI-agent PoC (week 5--7)}
\begin{description}
  \item[Onderzoekstechnieken:] Proof-of-concept ontwikkeling.
  \item[Doel:] Een AI-agent opzetten die zelfstandig rondloopt in de game en alles onderzoekt, met focus op dekking of mensachtig gedrag.
  \item[Deliverables:]
\end{description}
\begin{itemize}
  \item Een agent die draait met alle belangrijke configuraties ingesteld.
  \item Scripts waarmee we de experimenten eenvoudig opnieuw kunnen uitvoeren.
\end{itemize}

\subsection{Fase 4: Vergelijkingsexperiment (week 8--9)}
\begin{description}
  \item[Onderzoekstechnieken:] Experiment opzetten + vergelijkende studie.
  \item[Doel:] De mens, de scripted bot en de AI-agent op een eerlijke manier onder dezelfde omstandigheden testen.
\end{description}

{\bfseries Kwantitatieve meetwaarden}
\begin{itemize}
  \item {\bfseries Bugdetectie:} aantal unieke bugs per tijdsbudget.
  \item {\bfseries Time-to-first-bug:} hoe snel de eerste bug gevonden wordt.
  \item {\bfseries Testdekking:} gekeken naar de bezochte plekken, checkpoints en unieke gebeurtenissen.
  \item {\bfseries False positives:} meldingen van problemen die je later niet kunt reproduceren.
  \item {\bfseries Praktische kost:} tijd om alles op te zetten, trainingstijd en compute-gebruik.
\end{itemize}

\subsection{Fase 5: Analyse en rapportering (week 10--12)}
\begin{description}
  \item[Onderzoekstechnieken:] Data-analyse + validiteitscheck.
  \item[Doel:] De resultaten bekijken en er praktisch advies uit halen voor mid-size en indie-studio\textquoteright s.
  \item[Deliverables:]
\end{description}
\begin{itemize}
  \item Overzicht van de resultaten in tabellen en grafieken.
  \item Validiteitsbespreking (bv. hoe goed ``coverage'' werkt als maat, hoe toepasbaar het is op andere games).
  \item Conclusies en aanbevelingen over wanneer een AI-agent, een mens of een combinatie het best werkt.
\end{itemize}

\subsection{Tools}
{\bfseries Verwachte stack:} Voor tools gebruiken we Unity, ML-Agents, Python, CSV/JSON, Git en soms OpenCV voor visuele tests.

%---------- Verwachte resultaten ----------------------------------------------
\section{Verwacht resultaat, conclusie}%
\label{sec:verwachte_resultaten}

We verwachten dat een AI-agent in een afgebakende Unity-testopstelling vooral helpt om veel van de game te verkennen en reproduceerbare fouten te vinden. Een scripted bot blijft daarentegen vooral sterk voor stabiele regressietests. Mensen presteren relatief beter als het gaat om usability en het interpreteren van wat er in de game gebeurt. Daarom verwachten we dat AI-agents geen vervanging zijn voor handmatige QA. Ze kunnen wel een waardevolle aanvulling zijn in een hybride aanpak. Belangrijk is dat alles duidelijk wordt gerapporteerd, bijvoorbeeld met kernmetingen zoals het aantal bugs per tijdsbudget, de time-to-first-bug, testdekking, false positives en de praktische kosten.


